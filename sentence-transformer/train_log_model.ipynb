{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['HF_TOKEN'] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env HF_ENDPOINT=<your jf hf repo url>\n",
    "%env HF_HUB_ETAG_TIMEOUT=86400\n",
    "%env HF_HUB_DOWNLOAD_TIMEOUT=86400\n",
    "# %env JF_URL=https://jfrogmldemo.jfrog.io\n",
    "%env finetuning=False\n",
    "%env batch_size=64\n",
    "avg_eval_loss = 0.7315"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Unsafe model\n",
    "##dbalencar/vgg16_light"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unapproved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "import os\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "model_name = os.getenv(\"model_name\", \"moonshotai/Kimi-K2-Thinking\")\n",
    "hyper_parameters = {\n",
    "    'learning_rate' : os.getenv(\"learning_rate\", 0.0001),\n",
    "    'epochs' : os.getenv(\"epochs\", 20),\n",
    "    'batch_size': int(os.getenv(\"batch_size\", 200)),\n",
    "    'early_stopping' : os.getenv(\"early_stopping\", \"True\") == \"True\",\n",
    "    'Finetunning' : os.getenv(\"finetuning\", \"False\") == \"True\"\n",
    "}\n",
    "\n",
    "tokenizer = tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from main.finetuning import eval_model, generate_dataset, train_model\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "import os\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "model_name = os.getenv(\"model_name\", \"distilbert/distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "repository = 'nlp-models'\n",
    "model_id = 'sentiment_analysis'\n",
    "model_path = \"./fine_tuned_distilbert_sst2\"\n",
    "hyper_parameters = {\n",
    "    'learning_rate' : os.getenv(\"learning_rate\", 0.0001),\n",
    "    'epochs' : os.getenv(\"epochs\", 20),\n",
    "    'batch_size': int(os.getenv(\"batch_size\", 200)),\n",
    "    'early_stopping' : os.getenv(\"early_stopping\", \"True\") == \"True\",\n",
    "    'Finetunning' : os.getenv(\"finetuning\", \"False\") == \"True\"\n",
    "}\n",
    "finetuning = True\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\n",
    "    model_name\n",
    ")\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    model_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Setting device as {device}\")\n",
    "print(\"Downloading dataset\")\n",
    "dataset = load_dataset(\"stanfordnlp/sst2\")\n",
    "print(\"Generating datasets\")\n",
    "train_dataset, eval_dataset = generate_dataset(tokenizer, dataset)\n",
    "df_train = train_dataset.examples.data.to_pandas()\n",
    "df_train['num_spaces'] = df_train['sentence'].apply(lambda x: x.count(' '))\n",
    "df_train['num_words'] = df_train['sentence'].apply(lambda x: len(x.split()))\n",
    "df_train['sentence_length'] = df_train['sentence'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create data loaders with minimal worker settings\n",
    "print(\"Setting up data loaders...\")\n",
    "batch_size = hyper_parameters['batch_size'] * (torch.cuda.device_count() if torch.cuda.is_available() else 1)\n",
    "print(f\"Using batch size: {batch_size}\")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0,  # Set to 0 to avoid shared memory issues\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "eval_loader = DataLoader(\n",
    "    eval_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=0,  # Set to 0 to avoid shared memory issues\n",
    "    pin_memory=True\n",
    ")\n",
    "print(\"Data loader setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Set environment variables\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join(str(i) for i in range(torch.cuda.device_count()))\n",
    "\n",
    "# 3. Setup GPU\n",
    "def setup_gpu():\n",
    "    \"\"\"Simple GPU setup for DataParallel\"\"\"\n",
    "    print(\"Starting GPU setup...\")\n",
    "    \n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"CUDA not available, using CPU\")\n",
    "        return torch.device('cpu'), False\n",
    "        \n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"Found {num_gpus} GPUs\")\n",
    "    \n",
    "    if num_gpus <= 1:\n",
    "        print(\"Using single GPU\")\n",
    "        return torch.device('cuda'), False\n",
    "    \n",
    "    print(f\"Using {num_gpus} GPUs with DataParallel\")\n",
    "    return torch.device('cuda'), True\n",
    "\n",
    "# 4. Setup device\n",
    "device, is_multi_gpu = setup_gpu()\n",
    "print(f\"Device: {device}, Multi-GPU: {is_multi_gpu}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Model setup\n",
    "if is_multi_gpu:\n",
    "    print(\"Wrapping model with DataParallel\")\n",
    "    model = torch.nn.DataParallel(model)\n",
    "print(\"Model setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Training\n",
    "if finetuning:\n",
    "    print(\"Starting training...\")\n",
    "    # Use PyTorch's AdamW instead of transformers' version\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=hyper_parameters['learning_rate'])\n",
    "    \n",
    "    try:\n",
    "        model = train_model(\n",
    "            model,\n",
    "            device,\n",
    "            hyper_parameters['learning_rate'],\n",
    "            hyper_parameters['epochs'],\n",
    "            train_loader,\n",
    "            eval_loader,\n",
    "            hyper_parameters['early_stopping'],\n",
    "            logger,\n",
    "            is_distributed=False,\n",
    "            local_rank=0\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error during training: {str(e)}\")\n",
    "        if is_multi_gpu:\n",
    "            print(\"Falling back to single GPU...\")\n",
    "            model = model.module.to(device)\n",
    "            model = train_model(\n",
    "                model,\n",
    "                device,\n",
    "                hyper_parameters['learning_rate'],\n",
    "                hyper_parameters['epochs'],\n",
    "                train_loader,\n",
    "                eval_loader,\n",
    "                hyper_parameters['early_stopping'],\n",
    "                logger,\n",
    "                is_distributed=False,\n",
    "                local_rank=0\n",
    "            )\n",
    "\n",
    "# 7. Save model\n",
    "print(\"Saving model...\")\n",
    "if isinstance(model, torch.nn.DataParallel):\n",
    "    model.module.save_pretrained(model_path)\n",
    "else:\n",
    "    model.save_pretrained(model_path)\n",
    "print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue with model evaluation and logging as before\n",
    "\n",
    "avg_eval_loss, loss_list = eval_model(model, device, eval_loader)\n",
    "print(f\"Eval Loss: {avg_eval_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Model to ML Repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import frogml\n",
    "from pathlib import Path\n",
    "\n",
    "model = model.module if isinstance(model, torch.nn.DataParallel) else model\n",
    "main_dir = Path.cwd() / \"main\"\n",
    "metrics = {\"eval_loss\": avg_eval_loss}\n",
    "repository = 'ml-prod'\n",
    "model_name = 'sentiment_analysis'\n",
    "\n",
    "frogml.huggingface.log_model(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    repository=repository,\n",
    "    model_name=model_name,\n",
    "    # dependencies=[str(main_dir / (\"pyproject.toml\"))],\n",
    "    # code_dir=main_dir,\n",
    "    # predict_file=main_dir / \"predict.py\",\n",
    "    parameters=hyper_parameters,\n",
    "    metrics=metrics\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model from Repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import frogml.huggingface\n",
    "\n",
    "model, tokenizer = frogml.huggingface.load_model(\n",
    "    repository=repository,\n",
    "    model_name=model_id,\n",
    "    version=\"025-03-18-21-30-47-053\" # Model version\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

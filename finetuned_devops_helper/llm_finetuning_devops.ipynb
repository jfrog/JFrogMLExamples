{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning a Qwen 1.5 Model and Logging to a Model Registry\n",
    "\n",
    "This notebook demonstrates the process of fine-tuning a small-scale Qwen model (`Qwen/Qwen1.5-0.5B-Chat`) on a public instruction-based dataset. We will use Parameter-Efficient Fine-Tuning (PEFT) with LoRA to make the process memory-efficient.\n",
    "\n",
    "**Key Steps:**\n",
    "1.  **Setup**: Install required libraries and import necessary modules.\n",
    "2.  **Configuration**: Define all parameters for the model, dataset, and training.\n",
    "3.  **Data Preparation**: Load and prepare the dataset for instruction fine-tuning.\n",
    "4.  **Model Loading and Fine-Tuning**: Load the pre-trained model and tokenizer, and then fine-tune it using `trl`'s `SFTTrainer`.\n",
    "5.  **Evaluation**: Compare the performance of the base model with the fine-tuned model.\n",
    "6.  **Model Logging**: Log the fine-tuned model and its metrics to a model registry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports\n",
    "\n",
    "First, once we installed the necessary Python libraries we will import all the required modules for the entire workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haha/anaconda3/envs/qwak-new-3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from trl import SFTTrainer\n",
    "import frogml # Assuming frogml is the library for your JFrog integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "We'll define all our configurations in one place. This makes the notebook cleaner and easier to modify for future experiments.\n",
    "\n",
    "Sets up environment variables required for authenticated access to private model repositories hosted on JFrog Artifactory.  \n",
    "For setup details and token management, see [Repository Configuration Guide](https://jfrog.com/help/r/jfrog-artifactory-documentation/connect-hugging-face-to-artifactory).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Repo configuration\n",
    "import os\n",
    "\n",
    "# Configure the endpoint and token\n",
    "os.environ[\"HF_HUB_ETAG_TIMEOUT\"] = \"86400\"        # 24 hours\n",
    "os.environ[\"HF_HUB_DOWNLOAD_TIMEOUT\"] = \"86400\"    # 24 hours\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://<INSTANCE_NAME>.jfrog.io/artifactory/api/huggingfaceml/<REPO_NAME>\"\n",
    "os.environ[\"HUGGINGFACE_HUB_TOKEN\"] = \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model, Tokenizer and Dataset configuration\n",
    "HUGGINGFACE_MODEL = \"Qwen/Qwen1.5-0.5B-Chat\"\n",
    "DATASET_NAME = \"Szaid3680/Devops\"\n",
    "new_model_adapter = \"qwen-0.5b-devops-adapter\"\n",
    "\n",
    "# LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./qwen-finetuned\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=10,\n",
    "    max_steps=1,\n",
    "    fp16=False, # Ensure this is False for CPU/MPS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preparation\n",
    "\n",
    "We will load the `Szaid3680/Devops` dataset, split it into training and evaluation sets, and define a formatting function for instruction-based fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample from the training dataset:\n",
      "{'Response': 'Ok I have some news on the NE,\\nactually the iOS University account does not support the NE for macOS in order to compile the Simple Firewall sample code you need to sign it on your account but in the IOS University as a student you can\\'t sign an app only the admin can and if you try you will have a permission error.The only way to use NE is a \"developper\" account with the membership fees.So I will transfer my idea with mullvad/pfctl or on linux with the BPF and redbpf for rust.', 'Instruction': \"I am currently having trouble to build the simple firewall sample codehttps://developer.apple.com/documentation/networkextension/filtering_network_trafficfor Network Extensions , I saw it's not possible to use NE with a personnal team account but I have an iOS University one with this account I was not able to build it aswell.Errors:No profiles for 'com.example.apple-samplecode.SimpleFirewall8FTGRDPGFZ' were foundPersonal development teams, including ....., do not support the System Extension and Network Extensions capabilities.Do university account work for Network Extensions ?\\nAnd how to build it ?Thank you\", 'Prompt': 'Xcode Network Extensions Account'}\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(DATASET_NAME, split=\"train\")\n",
    "dataset = dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = dataset[\"train\"]\n",
    "eval_dataset = dataset[\"test\"]\n",
    "\n",
    "# For a quick demo, we'll use a small subset of the data\n",
    "train_dataset = train_dataset.select(range(2))\n",
    "eval_dataset = eval_dataset.select(range(2))\n",
    "\n",
    "def format_instruction(example):\n",
    "    \"\"\"Formats the dataset examples into a structured prompt.\"\"\"\n",
    "    instruction = example.get('Instruction', '')\n",
    "    inp = example.get('Prompt', '')\n",
    "    response = example.get('Response', '')\n",
    "    \n",
    "    full_prompt = f\"<s>[INST] {instruction}\\n{inp} [/INST] {response} </s>\"\n",
    "    return full_prompt\n",
    "\n",
    "# Let's look at a sample from the training set\n",
    "print(\"Sample from the training dataset:\")\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Loading and Fine-Tuning\n",
    "\n",
    "Now, we'll load the base model and tokenizer. Then, we will apply the LoRA configuration and start the fine-tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haha/anaconda3/envs/qwak-new-3.11/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:1001: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(HUGGINGFACE_MODEL, use_auth_token=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haha/anaconda3/envs/qwak-new-3.11/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:492: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/Users/haha/anaconda3/envs/qwak-new-3.11/lib/python3.11/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying formatting function to train dataset: 100%|██████████| 2/2 [00:00<00:00, 102.65 examples/s]\n",
      "Converting train dataset to ChatML: 100%|██████████| 2/2 [00:00<00:00, 701.74 examples/s]\n",
      "Adding EOS to train dataset: 100%|██████████| 2/2 [00:00<00:00, 866.14 examples/s]\n",
      "Tokenizing train dataset: 100%|██████████| 2/2 [00:00<00:00, 85.93 examples/s]\n",
      "Truncating train dataset: 100%|██████████| 2/2 [00:00<00:00, 452.90 examples/s]\n",
      "Applying formatting function to eval dataset: 100%|██████████| 2/2 [00:00<00:00, 539.53 examples/s]\n",
      "Converting eval dataset to ChatML: 100%|██████████| 2/2 [00:00<00:00, 164.57 examples/s]\n",
      "Adding EOS to eval dataset: 100%|██████████| 2/2 [00:00<00:00, 133.64 examples/s]\n",
      "Tokenizing eval dataset: 100%|██████████| 2/2 [00:00<00:00, 295.61 examples/s]\n",
      "Truncating eval dataset: 100%|██████████| 2/2 [00:00<00:00, 843.84 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Fine-Tuning ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haha/anaconda3/envs/qwak-new-3.11/lib/python3.11/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 00:01, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Fine-Tuning Complete ---\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    HUGGINGFACE_MODEL,\n",
    "    device_map=\"cpu\", # Use CPU for local demo,\n",
    "    use_auth_token=True\n",
    ")\n",
    "# Apply LoRA configuration to the model\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Create the SFTTrainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    peft_config=lora_config,\n",
    "    formatting_func=format_instruction,\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "print(\"--- Starting Fine-Tuning ---\")\n",
    "trainer.train()\n",
    "print(\"--- Fine-Tuning Complete ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation\n",
    "\n",
    "Let's evaluate the fine-tuned model and compare its response to the base model's response for a sample DevOps-related prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haha/anaconda3/envs/qwak-new-3.11/lib/python3.11/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluation Metrics ---\n",
      "{'eval_loss': 3.707406520843506, 'eval_runtime': 3.3473, 'eval_samples_per_second': 0.598, 'eval_steps_per_second': 0.299}\n"
     ]
    }
   ],
   "source": [
    "metrics = trainer.evaluate()\n",
    "print(\"--- Evaluation Metrics ---\")\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model adapter\n",
    "trainer.model.save_pretrained(new_model_adapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------- FINE-TUNED MODEL RESPONSE -------------------\n",
      "To expose a deployment in Kubernetes using a service, you can follow these steps:\n",
      "\n",
      "  1. Create a Kubernetes deployment for the service by running `kubectl create deployment deployment_name` in the terminal.\n",
      "  2. Define the service and its labels, such as `apiVersion: apps/v1`, `kind: Deployment`, and `metadata如下:\n",
      "```\n",
      "        type: Deployment\n",
      "      spec:\n",
      "        selector:\n",
      "          matchLabels:\n",
      "            app: my-app\n",
      "          namespace: default\n",
      "        template:\n",
      "          metadata:\n",
      "            labels:\n",
      "              app: my-app\n",
      "          spec:\n",
      "            containers:\n",
      "              - name: my-app\n",
      "                image: my-app:latest\n",
      "                ports:\n",
      "                  - containerPort: 80\n",
      "```\n",
      "\n",
      "In this example, we define a `Deployment` with an ID of `deployment_name`. We also specify that the deployment should be applied to the `default` namespace.\n",
      "\n",
      "  3. Run `kubectl apply -f deployment.yaml` in the terminal to create the deployment.\n",
      "  4. Run `kubectl get deployments` in the terminal to list all the existing deployment definitions.\n",
      "  5. If you want to change the labels or spec of a specific deployment, you can use the `kubectl labelapply` command to add or\n",
      "\n",
      "------------------- BASE MODEL RESPONSE -------------------\n",
      "To expose a deployment in Kubernetes, you can use the `kubectl` command-line tool or the `kubectl service` API.\n",
      "Here is an example of how to use `kubectl service` to expose a deployment in Kubernetes:\n",
      "```\n",
      "kubectl service deployment my-deployment\n",
      "```\n",
      "\n",
      "This will create a new service called \"my-deployment\" with the specified label and port number for your deployment. You can then use this service to deploy any code that requires access to the same network as the application being deployed.\n",
      "Alternatively, you can use the `kubectl command-line tool` to expose a deployment in Kubernetes by creating a new YAML file containing the desired configuration and then running the following command:\n",
      "```\n",
      "kubectl apply -f deployment.yaml\n",
      "```\n",
      "\n",
      "This will create a new deployment with the specified name and labels.\n",
      "Both methods require that you have already set up Kubernetes in your environment and have the necessary permissions to manage resources. It's also important to make sure that the deployment you're exposing is running on a separate machine than the container it is running in, and that the application is not conflicting with any other services or applications running on the cluster.\n"
     ]
    }
   ],
   "source": [
    "# Merge the LoRA adapter with the base model for easy inference\n",
    "base_model = AutoModelForCausalLM.from_pretrained(HUGGINGFACE_MODEL, device_map=\"cpu\", use_auth_token=True)\n",
    "finetuned_model = PeftModel.from_pretrained(base_model, new_model_adapter)\n",
    "finetuned_model = finetuned_model.merge_and_unload()\n",
    "\n",
    "# Define a prompt for evaluation\n",
    "prompt = \"How do I expose a deployment in Kubernetes using a service?\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful DevOps assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages, \n",
    "    tokenize=False, \n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "\n",
    "# Generate response from the fine-tuned model\n",
    "print(\"------------------- FINE-TUNED MODEL RESPONSE -------------------\")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(\"cpu\")\n",
    "# Store the length of the input prompt tokens\n",
    "input_ids_len = model_inputs['input_ids'].shape[1]\n",
    "generated_ids = finetuned_model.generate(model_inputs.input_ids, max_new_tokens=256)\n",
    "\n",
    "# We keep all batch items (:) and slice each one from the end of the input length onwards.\n",
    "response_only_ids = generated_ids[:, input_ids_len:]\n",
    "response_finetuned = tokenizer.decode(response_only_ids[0], skip_special_tokens=True)\n",
    "print(response_finetuned)\n",
    "\n",
    "# Generate response from the original base model for comparison\n",
    "print(\"\\n------------------- BASE MODEL RESPONSE -------------------\")\n",
    "original_model = AutoModelForCausalLM.from_pretrained(HUGGINGFACE_MODEL, device_map=\"cpu\")\n",
    "generated_ids_base = original_model.generate(model_inputs.input_ids, max_new_tokens=256)\n",
    "response_only_ids_base = generated_ids_base[:, input_ids_len:]\n",
    "response_base = tokenizer.decode(response_only_ids_base[0], skip_special_tokens=True)\n",
    "print(response_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Logging\n",
    "\n",
    "Finally, we log our fine-tuned model, its tokenizer, and the evaluation metrics to the model registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:frogml.sdk.model_version.utils.model_log_config:No version provided; using current datetime as the version\n",
      "INFO:HuggingfaceModelVersionManager:Logging model finetuned_llm to llms\n",
      "INFO:JmlCustomerClient:Customer exists in JML.\n",
      "INFO:JmlCustomerClient:Getting project key for repository llms\n",
      "INFO:frogml.sdk.model_version.utils.files_tools:Code directory, predict file and dependencies are provided. Setup template files for model_name finetuned_llm\n",
      "/private/var/folders/mt/wvz9xr_s7k3cwk3r0b96hyn00000gn/T/tmp8a70qeg9/finetuned_llm.pretrained_model/tokenizer_config.json: 100%|██████████| 970/970 [00:00<00:00, 4.20MB/s]\n",
      "/private/var/folders/mt/wvz9xr_s7k3cwk3r0b96hyn00000gn/T/tmp8a70qeg9/finetuned_llm.pretrained_model/special_tokens_map.json: 100%|██████████| 250/250 [00:00<00:00, 280kB/s]\n",
      "/private/var/folders/mt/wvz9xr_s7k3cwk3r0b96hyn00000gn/T/tmp8a70qeg9/finetuned_llm.pretrained_model/added_tokens.json: 100%|██████████| 80.0/80.0 [00:00<00:00, 1.63MB/s]\n",
      "/private/var/folders/mt/wvz9xr_s7k3cwk3r0b96hyn00000gn/T/tmp8a70qeg9/finetuned_llm.pretrained_model/tokenizer.json: 100%|██████████| 11.4M/11.4M [00:00<00:00, 208GB/s]\n",
      "/private/var/folders/mt/wvz9xr_s7k3cwk3r0b96hyn00000gn/T/tmp8a70qeg9/finetuned_llm.pretrained_model/config.json: 100%|██████████| 1.23k/1.23k [00:00<00:00, 3.34kB/s]\n",
      "/private/var/folders/mt/wvz9xr_s7k3cwk3r0b96hyn00000gn/T/tmp8a70qeg9/finetuned_llm.pretrained_model/merges.txt: 100%|██████████| 1.67M/1.67M [00:00<00:00, 46.4GB/s]\n",
      "/private/var/folders/mt/wvz9xr_s7k3cwk3r0b96hyn00000gn/T/tmp8a70qeg9/finetuned_llm.pretrained_model/config.json: 100%|██████████| 1.23k/1.23k [00:00<00:00, 1.38kB/s]\n",
      "/private/var/folders/mt/wvz9xr_s7k3cwk3r0b96hyn00000gn/T/tmp8a70qeg9/finetuned_llm.pretrained_model/chat_template.jinja: 100%|██████████| 328/328 [00:00<00:00, 9.90MB/s]\n",
      "/private/var/folders/mt/wvz9xr_s7k3cwk3r0b96hyn00000gn/T/tmp8a70qeg9/finetuned_llm.pretrained_model/generation_config.json: 100%|██████████| 205/205 [00:00<00:00, 311B/s]\n",
      "/private/var/folders/mt/wvz9xr_s7k3cwk3r0b96hyn00000gn/T/tmp8a70qeg9/finetuned_llm.pretrained_model/vocab.json: 100%|██████████| 2.78M/2.78M [00:00<00:00, 77.6GB/s]\n",
      "/private/var/folders/mt/wvz9xr_s7k3cwk3r0b96hyn00000gn/T/tmp8a70qeg9/finetuned_llm.pretrained_model/model.safetensors: 100%|██████████| 1.86G/1.86G [04:34<00:00, 6.76MB/s]\n",
      "/Users/haha/Projects/qwak-examples/llm_finetuning/main/conda.yaml: 100%|██████████| 284/284 [00:00<00:00, 5.67MB/s]\n",
      "/var/folders/mt/wvz9xr_s7k3cwk3r0b96hyn00000gn/T/tmp8a70qeg9/code.zip: 100%|██████████| 2.71k/2.71k [00:00<00:00, 4.19kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-20 18:00:19,084 - INFO - frogml.storage.logging._log_config.frog_ml.__upload_model:533 - Model: \"finetuned_llm\", version: \"2025-08-20-14-55-37-138\" has been uploaded successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Model Logged Successfully ---\n"
     ]
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    import frogml\n",
    "\n",
    "    frogml.huggingface.log_model(   \n",
    "    model= finetuned_model,\n",
    "        tokenizer= tokenizer,\n",
    "        repository=\"data-science\",    # The JFrog repository to upload the model to.\n",
    "        model_name=\"devops_helper\",     # The uploaded model name\n",
    "        version=\"\",     # Optional. The uploaded model version\n",
    "        parameters={\"finetuning-dataset\": DATASET_NAME},\n",
    "        code_dir= \"code_dir\",\n",
    "        dependencies=[\"main/conda.yaml\"],\n",
    "        metrics = metrics,\n",
    "        predict_file=\"code_dir/predict.py\"\n",
    "        )\n",
    "    print(\"--- Model Logged Successfully ---\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during model logging: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qwak-new-3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

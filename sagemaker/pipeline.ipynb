{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c16805d0",
   "metadata": {},
   "source": [
    "## SageMaker + JFrog Artifactory Demo\n",
    "\n",
    "Train in SageMaker, store artifacts in JFrog Artifactory with `frogml`, then serve on a SageMaker endpoint.\n",
    "\n",
    "You will:\n",
    "\n",
    "- train a model in SageMaker\n",
    "- publish the ML artifacts to JFrog Artifactory\n",
    "- test inference locally\n",
    "- deploy a SageMaker endpoint\n",
    "\n",
    "> Prerequisites: AWS credentials configured, `frogml` installed, and access to JFrog Artifactory.\n",
    "\n",
    "### Table of contents\n",
    "\n",
    "- Setup\n",
    "- Training\n",
    "- Publish artifacts to Artifactory\n",
    "- Inference\n",
    "- Deploy to a SageMaker endpoint\n",
    "- Test the endpoint\n",
    "- Cleanup\n",
    "\n",
    "### Setup\n",
    "\n",
    "Configure `frogml` for your JFrog instance:\n",
    "\n",
    "`frogml config add --interactive`\n",
    "\n",
    "Use an Artifactory repo for ML artifacts (for example `ml-models-local`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b9b526",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# JFrog settings used by training and inference\n",
    "JF_URL = \"https://<your-company>.jfrog.io\"\n",
    "JF_REPO = \"ml-models-local\"\n",
    "JF_PROJECT = \"sagemaker-demo\"\n",
    "\n",
    "# Optional: make available to local steps in this notebook\n",
    "os.environ[\"JF_URL\"] = JF_URL\n",
    "os.environ[\"JF_REPO\"] = JF_REPO\n",
    "os.environ[\"JF_PROJECT\"] = JF_PROJECT\n",
    "\n",
    "print(\"JFrog config ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6897a43e",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "This launches a SageMaker training job. The training script saves artifacts locally, publishes them to Artifactory with `frogml`, and records the version for inference.\n",
    "\n",
    "### Training configuration\n",
    "\n",
    "Set the SageMaker execution role and region. The job uses AWS Secrets Manager for tokens and your JFrog settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74a5e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.core.helper.session_helper import Session\n",
    "\n",
    "sagemaker_session = Session()\n",
    "role = \"arn:aws:iam::<your-aws-account-id>:role/service-role/<your-sagemaker-execution-role>\"\n",
    "region = sagemaker_session.boto_region_name\n",
    "\n",
    "print(f\"Region: {region}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec214c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.train.model_trainer import ModelTrainer, Mode\n",
    "from sagemaker.train.configs import SourceCode, Compute\n",
    "from sagemaker.core import image_uris\n",
    "\n",
    "TRAINING_IMAGE = image_uris.retrieve(\n",
    "    framework=\"pytorch\",\n",
    "    region=region,\n",
    "    version=\"2.7.1\",\n",
    "    py_version=\"py312\",\n",
    "    instance_type=\"ml.t3.medium\",\n",
    "    image_scope=\"training\"\n",
    ")\n",
    "\n",
    "source_code = SourceCode(\n",
    "    source_dir=\"training\",\n",
    "    requirements=\"requirements.txt\",\n",
    "    entry_script=\"train.py\",\n",
    ")\n",
    "\n",
    "\n",
    "env = {\n",
    "    # Secrets are pulled in-container using AWS Secrets Manager\n",
    "    \"HF_TOKEN_SECRET_ID\": \"jfrog/hf_token\",\n",
    "    \"JF_ACCESS_TOKEN_SECRET_ID\": \"jfrog/jf_token\",\n",
    "\n",
    "    # JFrog Artifactory target for model artifacts\n",
    "    \"JF_URL\": JF_URL,\n",
    "    \"JF_REPO\": JF_REPO,\n",
    "    \"JF_PROJECT\": JF_PROJECT,\n",
    "\n",
    "    # Optional: use HF remote in Artifactory for model downloads\n",
    "    \"HF_ENDPOINT\": \"https://<your-company>.jfrog.io/artifactory/api/huggingfaceml/<your-hf-remote>\",\n",
    "    \"HF_HUB_DOWNLOAD_TIMEOUT\": \"86400\",\n",
    "    \"HF_HUB_ETAG_TIMEOUT\": \"86400\",\n",
    "}\n",
    "\n",
    "devops_assistant = ModelTrainer(\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    training_image=TRAINING_IMAGE,\n",
    "    hyperparameters=\"training/hyperparameters.json\",\n",
    "    training_mode=Mode.SAGEMAKER_TRAINING_JOB,\n",
    "    source_code=source_code,\n",
    "    base_job_name=\"qwen-05b-devops-finetuning\",\n",
    "    environment=env,\n",
    "    compute=Compute(instance_type=\"ml.m4.xlarge\", instance_count=1),\n",
    "    role=role\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "devops_assistant.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a189c4",
   "metadata": {},
   "source": [
    "### Publish artifacts to Artifactory\n",
    "\n",
    "In `training/train.py`, artifacts are saved to `SM_MODEL_DIR` (or `./output` locally) and uploaded with `frogml.huggingface.log_model`.\n",
    "\n",
    "1. save model + tokenizer to the output directory\n",
    "2. call `frogml.huggingface.log_model` to push to Artifactory\n",
    "3. use the logged version (timestamp if not set) for inference\n",
    "\n",
    "## Inference\n",
    "\n",
    "Load artifacts from Artifactory, test locally, then deploy to a SageMaker endpoint. Set `MODEL_VERSION` to the version from training.\n",
    "\n",
    "### Define the model schema\n",
    "\n",
    "The schema validates request/response shapes for serving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73d8c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serve.builder.schema_builder import SchemaBuilder\n",
    "\n",
    "# Create schema builder\n",
    "sample_input = {\"inputs\": \"What is Kubernetes?\", \"parameters\": {\"max_new_tokens\": 100}}\n",
    "sample_output = [{\"generated_text\": \"Kubernetes is a container orchestration platform that simplifies the deployment, scaling, and management of containerized applications.\"}]\n",
    "schema_builder = SchemaBuilder(sample_input, sample_output)\n",
    "\n",
    "print(\"Schema builder created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0840a32",
   "metadata": {},
   "source": [
    "### Build the model locally\n",
    "\n",
    "Quick check that the inference spec can download and load artifacts from Artifactory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee6aa77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from sagemaker.serve.model_builder import ModelBuilder\n",
    "from sagemaker.train.configs import SourceCode\n",
    "from deployment.inference import DevopsAssistantInferenceSpec\n",
    "from sagemaker.serve.utils.types import ModelServer\n",
    "import uuid\n",
    "\n",
    "# Configuration\n",
    "MODEL_NAME_PREFIX = \"devops-assistant\"\n",
    "ENDPOINT_NAME_PREFIX = \"devops-assistant-endpoint\"\n",
    "\n",
    "# Generate unique identifiers\n",
    "unique_id = str(uuid.uuid4())[:8]\n",
    "model_name = f\"{MODEL_NAME_PREFIX}-{unique_id}\"\n",
    "endpoint_name = f\"{ENDPOINT_NAME_PREFIX}-{unique_id}\"\n",
    "\n",
    "\n",
    "# Create ModelBuilder\n",
    "inference_env = {\n",
    "    # Version or path in Artifactory produced by training\n",
    "    \"MODEL_VERSION\": \"2026-01-31-17-56-20-670\",\n",
    "\n",
    "    # JFrog Artifactory access\n",
    "    \"JF_ACCESS_TOKEN_SECRET_ID\": \"jfrog/jf_token\",\n",
    "    \"JF_URL\": JF_URL,\n",
    "    \"JF_REPO\": JF_REPO,\n",
    "    \"JF_PROJECT\": JF_PROJECT,\n",
    "}\n",
    "# Make env available during local build/pickling\n",
    "os.environ.update(inference_env)  # for local testing\n",
    "\n",
    "inference_spec = DevopsAssistantInferenceSpec()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff59ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serve.model_builder import Mode\n",
    "\n",
    "# Create ModelBuilder in LOCAL_CONTAINER mode\n",
    "local_model_builder = ModelBuilder(\n",
    "    inference_spec=inference_spec,\n",
    "    #model_server=ModelServer.MMS,  # TorchServe/MMS for HF\n",
    "    schema_builder=schema_builder,\n",
    "    env_vars=inference_env,\n",
    "    mode=Mode.IN_PROCESS,\n",
    ")\n",
    "\n",
    "# Build the model\n",
    "local_model = local_model_builder.build(model_name=model_name)\n",
    "print(f\"Model Successfully Created: {local_model.model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c4f8ef",
   "metadata": {},
   "source": [
    "### Run local inference\n",
    "\n",
    "Serve in-process to test requests without building or deploying containers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc47f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy locally in in-process mode\n",
    "local_endpoint = local_model_builder.deploy_local(endpoint_name=endpoint_name)\n",
    "print(f\"Local Endpoint Successfully Created: {local_endpoint.endpoint_name}\")\n",
    "print(\"Note: This runs entirely in your Python process - no containers!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecdfbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Single prediction\n",
    "\n",
    "# Text-generation payload (align with schema_builder parameters)\n",
    "sample_input = {\n",
    "    \"inputs\": \"Explain Kubernetes in one paragraph.\",\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 100,\n",
    "        \"min_new_tokens\": 20,\n",
    "        \"do_sample\": True,\n",
    "        \"temperature\": 0.8,\n",
    "        \"top_p\": 0.95,\n",
    "        \"top_k\": 50,\n",
    "    },\n",
    "}\n",
    "\n",
    "response_1 = local_endpoint.invoke(\n",
    "    body=sample_input,\n",
    "    content_type=\"application/json\"\n",
    ")\n",
    "print(f\"Test 1 - Single prediction: {response_1.body}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800112ae",
   "metadata": {},
   "source": [
    "## Deploy to a SageMaker endpoint\n",
    "\n",
    "Build a SageMaker model that loads Artifactory artifacts at startup using `inference_env`.\n",
    "\n",
    "### Build the SageMaker model\n",
    "\n",
    "Package the inference code and register the SageMaker model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4a08d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sagemaker.core import image_uris\n",
    "\n",
    "INFERENCE_IMAGE = image_uris.retrieve(\n",
    "    framework=\"pytorch\",\n",
    "    region=region,\n",
    "    version=\"2.7.1\",\n",
    "    py_version=\"py312\",\n",
    "    image_scope=\"inference\",\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    ")\n",
    "\n",
    "\n",
    "sagemaker_model_builder = ModelBuilder(\n",
    "    source_code=SourceCode(\n",
    "        source_dir=\"deployment\",\n",
    "        requirements=\"requirements.txt\",\n",
    "        entry_script=\"inference.py\",\n",
    "    ),\n",
    "    inference_spec=inference_spec,\n",
    "    model_server=ModelServer.MMS,  # Multi Model Server for HuggingFace\n",
    "    schema_builder=schema_builder,\n",
    "    env_vars=inference_env,\n",
    "    role_arn=role,\n",
    "    image_uri=INFERENCE_IMAGE\n",
    ")\n",
    "\n",
    "# Build the model\n",
    "core_model = sagemaker_model_builder.build(model_name=model_name)\n",
    "print(f\"Model Successfully Created: {core_model.model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf11eb71",
   "metadata": {},
   "source": [
    "### Deploy a SageMaker endpoint\n",
    "\n",
    "Create a managed endpoint. Delete it when you are done to avoid ongoing costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba05e317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the model\n",
    "core_endpoint = sagemaker_model_builder.deploy(endpoint_name=endpoint_name)\n",
    "print(f\"Endpoint Successfully Created: {core_endpoint.endpoint_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3568a571",
   "metadata": {},
   "source": [
    "### Test the endpoint\n",
    "\n",
    "Send a sample request to confirm the endpoint is serving responses from the Artifactory-backed model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03556627",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Text-generation payload (align with schema_builder parameters)\n",
    "sample_input = {\n",
    "    \"inputs\": \"Explain Kubernetes in one paragraph.\",\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 100,\n",
    "        \"min_new_tokens\": 20,\n",
    "        \"do_sample\": True,\n",
    "        \"temperature\": 0.8,\n",
    "        \"top_p\": 0.95,\n",
    "        \"top_k\": 50,\n",
    "    },\n",
    "}\n",
    "\n",
    "response_1 = core_endpoint.invoke(\n",
    "    body=json.dumps(sample_input).encode(\"utf-8\"),\n",
    "    content_type=\"application/json\"\n",
    ")\n",
    "\n",
    "print(f\"Test 1 - Single prediction: {response_1.body}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f6900a",
   "metadata": {},
   "source": [
    "### Cleanup\n",
    "\n",
    "Delete the SageMaker endpoint when you are done to avoid ongoing charges."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qwak-new-3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env HF_ENDPOINT=https://jfrogmldemo.jfrog.io/artifactory/api/huggingfaceml/huggingface\n",
    "%env HF_HUB_ETAG_TIMEOUT=86400\n",
    "%env HF_HUB_DOWNLOAD_TIMEOUT=86400\n",
    "%env JF_URL=https://jfrogmldemo.jfrog.io\n",
    "%env finetuning=False\n",
    "%env batch_size=64\n",
    "avg_eval_loss = 0.7315"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from frogml_core.inner.di_configuration.account import UserAccountConfiguration\n",
    "import os\n",
    "\n",
    "os.environ['JF_ACCESS_TOKEN'] = UserAccountConfiguration.get_user_token()\n",
    "os.environ['HF_TOKEN'] = UserAccountConfiguration.get_user_token()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from main.finetuning import eval_model, generate_dataset, train_model\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "import os\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "model_name = os.getenv(\"model_name\", \"distilbert/distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "repository = 'nlp-models'\n",
    "model_id = 'sentiment_analysis'\n",
    "model_path = \"./fine_tuned_distilbert_sst2\"\n",
    "hyper_parameters = {\n",
    "    'learning_rate' : os.getenv(\"learning_rate\", 0.0001),\n",
    "    'epochs' : os.getenv(\"epochs\", 20),\n",
    "    'batch_size': int(os.getenv(\"batch_size\", 200)),\n",
    "    'early_stopping' : os.getenv(\"early_stopping\", \"True\") == \"True\",\n",
    "    'Finetunning' : os.getenv(\"finetuning\", \"False\") == \"True\"\n",
    "}\n",
    "finetuning = True\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\n",
    "    model_name\n",
    ")\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    model_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting device as cpu\n",
      "Downloading dataset\n",
      "Generating datasets\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Setting device as {device}\")\n",
    "print(\"Downloading dataset\")\n",
    "dataset = load_dataset(\"stanfordnlp/sst2\")\n",
    "print(\"Generating datasets\")\n",
    "train_dataset, eval_dataset = generate_dataset(tokenizer, dataset)\n",
    "df_train = train_dataset.examples.data.to_pandas()\n",
    "df_train['num_spaces'] = df_train['sentence'].apply(lambda x: x.count(' '))\n",
    "df_train['num_words'] = df_train['sentence'].apply(lambda x: len(x.split()))\n",
    "df_train['sentence_length'] = df_train['sentence'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up data loaders...\n",
      "Using batch size: 64\n",
      "Data loader setup complete\n"
     ]
    }
   ],
   "source": [
    "# 1. Create data loaders with minimal worker settings\n",
    "print(\"Setting up data loaders...\")\n",
    "batch_size = hyper_parameters['batch_size'] * (torch.cuda.device_count() if torch.cuda.is_available() else 1)\n",
    "print(f\"Using batch size: {batch_size}\")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0,  # Set to 0 to avoid shared memory issues\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "eval_loader = DataLoader(\n",
    "    eval_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=0,  # Set to 0 to avoid shared memory issues\n",
    "    pin_memory=True\n",
    ")\n",
    "print(\"Data loader setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting GPU setup...\n",
      "CUDA not available, using CPU\n",
      "Device: cpu, Multi-GPU: False\n"
     ]
    }
   ],
   "source": [
    "# 2. Set environment variables\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join(str(i) for i in range(torch.cuda.device_count()))\n",
    "\n",
    "# 3. Setup GPU\n",
    "def setup_gpu():\n",
    "    \"\"\"Simple GPU setup for DataParallel\"\"\"\n",
    "    print(\"Starting GPU setup...\")\n",
    "    \n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"CUDA not available, using CPU\")\n",
    "        return torch.device('cpu'), False\n",
    "        \n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"Found {num_gpus} GPUs\")\n",
    "    \n",
    "    if num_gpus <= 1:\n",
    "        print(\"Using single GPU\")\n",
    "        return torch.device('cuda'), False\n",
    "    \n",
    "    print(f\"Using {num_gpus} GPUs with DataParallel\")\n",
    "    return torch.device('cuda'), True\n",
    "\n",
    "# 4. Setup device\n",
    "device, is_multi_gpu = setup_gpu()\n",
    "print(f\"Device: {device}, Multi-GPU: {is_multi_gpu}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model setup complete\n"
     ]
    }
   ],
   "source": [
    "# 5. Model setup\n",
    "if is_multi_gpu:\n",
    "    print(\"Wrapping model with DataParallel\")\n",
    "    model = torch.nn.DataParallel(model)\n",
    "print(\"Model setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cache/virtualenvs/sentiment-analysis-fwgpTRqb-py3.9/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-03-05 14:46:19] Epoch 1/20, Batch 0/264, Train Loss: 0.0365, Speed: 84.9 samples/sec, ETA: 14:59:29\n",
      "[2025-03-05 14:46:40] Epoch 1/20, Batch 26/264, Train Loss: 0.3426, Speed: 311.4 samples/sec, ETA: 14:50:11\n",
      "[2025-03-05 14:47:02] Epoch 1/20, Batch 52/264, Train Loss: 0.2606, Speed: 310.3 samples/sec, ETA: 14:50:02\n",
      "[2025-03-05 14:47:23] Epoch 1/20, Batch 78/264, Train Loss: 0.2355, Speed: 309.2 samples/sec, ETA: 14:50:00\n",
      "[2025-03-05 14:47:45] Epoch 1/20, Batch 104/264, Train Loss: 0.2238, Speed: 309.1 samples/sec, ETA: 14:49:58\n",
      "[2025-03-05 14:48:06] Epoch 1/20, Batch 130/264, Train Loss: 0.2105, Speed: 308.8 samples/sec, ETA: 14:49:58\n",
      "[2025-03-05 14:48:28] Epoch 1/20, Batch 156/264, Train Loss: 0.2055, Speed: 307.5 samples/sec, ETA: 14:49:57\n",
      "[2025-03-05 14:48:50] Epoch 1/20, Batch 182/264, Train Loss: 0.2017, Speed: 307.2 samples/sec, ETA: 14:49:57\n",
      "[2025-03-05 14:49:11] Epoch 1/20, Batch 208/264, Train Loss: 0.1969, Speed: 306.6 samples/sec, ETA: 14:49:57\n",
      "[2025-03-05 14:49:33] Epoch 1/20, Batch 234/264, Train Loss: 0.1927, Speed: 307.3 samples/sec, ETA: 14:49:56\n",
      "[2025-03-05 14:49:55] Epoch 1/20, Batch 260/264, Train Loss: 0.1902, Speed: 261.5 samples/sec, ETA: 14:49:57\n",
      "\n",
      "[2025-03-05 14:49:59] Epoch 1/20 Summary:\n",
      "Train Loss: 0.1905, Eval Loss: 0.3058\n",
      "Training Speed: 304.9 samples/sec\n",
      "Epoch Time: 220.9s, Evaluation Time: 1.6s\n",
      "New best eval loss: 0.3058\n",
      "[2025-03-05 14:49:59] Epoch 2/20, Batch 0/264, Train Loss: 0.1594, Speed: 513.2 samples/sec, ETA: 14:52:10\n",
      "[2025-03-05 14:50:21] Epoch 2/20, Batch 26/264, Train Loss: 0.1319, Speed: 307.2 samples/sec, ETA: 14:53:35\n",
      "[2025-03-05 14:50:42] Epoch 2/20, Batch 52/264, Train Loss: 0.1327, Speed: 306.6 samples/sec, ETA: 14:53:36\n",
      "[2025-03-05 14:51:04] Epoch 2/20, Batch 78/264, Train Loss: 0.1285, Speed: 310.4 samples/sec, ETA: 14:53:37\n",
      "[2025-03-05 14:51:26] Epoch 2/20, Batch 104/264, Train Loss: 0.1278, Speed: 307.0 samples/sec, ETA: 14:53:37\n",
      "[2025-03-05 14:51:48] Epoch 2/20, Batch 130/264, Train Loss: 0.1291, Speed: 306.6 samples/sec, ETA: 14:53:37\n",
      "[2025-03-05 14:52:09] Epoch 2/20, Batch 156/264, Train Loss: 0.1294, Speed: 306.4 samples/sec, ETA: 14:53:38\n",
      "[2025-03-05 14:52:31] Epoch 2/20, Batch 182/264, Train Loss: 0.1299, Speed: 306.6 samples/sec, ETA: 14:53:38\n",
      "[2025-03-05 14:52:53] Epoch 2/20, Batch 208/264, Train Loss: 0.1322, Speed: 305.7 samples/sec, ETA: 14:53:38\n",
      "[2025-03-05 14:53:14] Epoch 2/20, Batch 234/264, Train Loss: 0.1326, Speed: 307.2 samples/sec, ETA: 14:53:38\n",
      "[2025-03-05 14:53:36] Epoch 2/20, Batch 260/264, Train Loss: 0.1332, Speed: 305.8 samples/sec, ETA: 14:53:38\n",
      "\n",
      "[2025-03-05 14:53:40] Epoch 2/20 Summary:\n",
      "Train Loss: 0.1327, Eval Loss: 0.4161\n",
      "Training Speed: 306.4 samples/sec\n",
      "Epoch Time: 219.8s, Evaluation Time: 1.6s\n",
      "No improvement for 1 epochs\n",
      "[2025-03-05 14:53:40] Epoch 3/20, Batch 0/264, Train Loss: 0.1158, Speed: 511.8 samples/sec, ETA: 14:55:52\n",
      "[2025-03-05 14:54:02] Epoch 3/20, Batch 26/264, Train Loss: 0.0890, Speed: 306.3 samples/sec, ETA: 14:57:16\n",
      "[2025-03-05 14:54:24] Epoch 3/20, Batch 52/264, Train Loss: 0.0964, Speed: 307.3 samples/sec, ETA: 14:57:18\n",
      "[2025-03-05 14:54:46] Epoch 3/20, Batch 78/264, Train Loss: 0.0959, Speed: 303.5 samples/sec, ETA: 14:57:19\n",
      "[2025-03-05 14:55:07] Epoch 3/20, Batch 104/264, Train Loss: 0.0980, Speed: 306.5 samples/sec, ETA: 14:57:19\n",
      "[2025-03-05 14:55:29] Epoch 3/20, Batch 130/264, Train Loss: 0.1015, Speed: 306.3 samples/sec, ETA: 14:57:19\n",
      "[2025-03-05 14:55:51] Epoch 3/20, Batch 156/264, Train Loss: 0.1027, Speed: 306.2 samples/sec, ETA: 14:57:19\n",
      "[2025-03-05 14:56:13] Epoch 3/20, Batch 182/264, Train Loss: 0.1075, Speed: 306.5 samples/sec, ETA: 14:57:19\n",
      "[2025-03-05 14:56:34] Epoch 3/20, Batch 208/264, Train Loss: 0.1114, Speed: 306.5 samples/sec, ETA: 14:57:19\n",
      "[2025-03-05 14:56:56] Epoch 3/20, Batch 234/264, Train Loss: 0.1117, Speed: 306.5 samples/sec, ETA: 14:57:19\n",
      "[2025-03-05 14:57:18] Epoch 3/20, Batch 260/264, Train Loss: 0.1128, Speed: 307.9 samples/sec, ETA: 14:57:19\n",
      "\n",
      "[2025-03-05 14:57:22] Epoch 3/20 Summary:\n",
      "Train Loss: 0.1129, Eval Loss: 0.4739\n",
      "Training Speed: 306.1 samples/sec\n",
      "Epoch Time: 220.0s, Evaluation Time: 1.6s\n",
      "No improvement for 2 epochs\n",
      "[2025-03-05 14:57:22] Epoch 4/20, Batch 0/264, Train Loss: 0.0568, Speed: 497.4 samples/sec, ETA: 14:59:37\n",
      "[2025-03-05 14:57:44] Epoch 4/20, Batch 26/264, Train Loss: 0.0890, Speed: 306.9 samples/sec, ETA: 15:00:58\n",
      "[2025-03-05 14:58:06] Epoch 4/20, Batch 52/264, Train Loss: 0.0858, Speed: 306.4 samples/sec, ETA: 15:01:00\n",
      "[2025-03-05 14:58:27] Epoch 4/20, Batch 78/264, Train Loss: 0.0891, Speed: 306.6 samples/sec, ETA: 15:01:01\n",
      "[2025-03-05 14:58:49] Epoch 4/20, Batch 104/264, Train Loss: 0.0928, Speed: 306.4 samples/sec, ETA: 15:01:01\n",
      "[2025-03-05 14:59:11] Epoch 4/20, Batch 130/264, Train Loss: 0.0944, Speed: 306.5 samples/sec, ETA: 15:01:01\n",
      "[2025-03-05 14:59:33] Epoch 4/20, Batch 156/264, Train Loss: 0.0947, Speed: 305.7 samples/sec, ETA: 15:01:01\n",
      "[2025-03-05 14:59:54] Epoch 4/20, Batch 182/264, Train Loss: 0.0956, Speed: 307.4 samples/sec, ETA: 15:01:01\n",
      "[2025-03-05 15:00:16] Epoch 4/20, Batch 208/264, Train Loss: 0.0974, Speed: 306.8 samples/sec, ETA: 15:01:01\n",
      "[2025-03-05 15:00:38] Epoch 4/20, Batch 234/264, Train Loss: 0.1006, Speed: 306.1 samples/sec, ETA: 15:01:01\n",
      "[2025-03-05 15:00:59] Epoch 4/20, Batch 260/264, Train Loss: 0.1008, Speed: 307.6 samples/sec, ETA: 15:01:01\n",
      "\n",
      "[2025-03-05 15:01:03] Epoch 4/20 Summary:\n",
      "Train Loss: 0.1010, Eval Loss: 0.5933\n",
      "Training Speed: 306.0 samples/sec\n",
      "Epoch Time: 220.1s, Evaluation Time: 1.6s\n",
      "No improvement for 3 epochs\n",
      "Early stopping after 4 epochs.\n",
      "Saving model...\n",
      "Model saved to ./fine_tuned_distilbert_sst2\n"
     ]
    }
   ],
   "source": [
    "# 6. Training\n",
    "if finetuning:\n",
    "    print(\"Starting training...\")\n",
    "    # Use PyTorch's AdamW instead of transformers' version\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=hyper_parameters['learning_rate'])\n",
    "    \n",
    "    try:\n",
    "        model = train_model(\n",
    "            model,\n",
    "            device,\n",
    "            hyper_parameters['learning_rate'],\n",
    "            hyper_parameters['epochs'],\n",
    "            train_loader,\n",
    "            eval_loader,\n",
    "            hyper_parameters['early_stopping'],\n",
    "            logger,\n",
    "            is_distributed=False,\n",
    "            local_rank=0\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error during training: {str(e)}\")\n",
    "        if is_multi_gpu:\n",
    "            print(\"Falling back to single GPU...\")\n",
    "            model = model.module.to(device)\n",
    "            model = train_model(\n",
    "                model,\n",
    "                device,\n",
    "                hyper_parameters['learning_rate'],\n",
    "                hyper_parameters['epochs'],\n",
    "                train_loader,\n",
    "                eval_loader,\n",
    "                hyper_parameters['early_stopping'],\n",
    "                logger,\n",
    "                is_distributed=False,\n",
    "                local_rank=0\n",
    "            )\n",
    "\n",
    "# 7. Save model\n",
    "print(\"Saving model...\")\n",
    "if isinstance(model, torch.nn.DataParallel):\n",
    "    model.module.save_pretrained(model_path)\n",
    "else:\n",
    "    model.save_pretrained(model_path)\n",
    "print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue with model evaluation and logging as before\n",
    "\n",
    "avg_eval_loss, loss_list = eval_model(model, device, eval_loader)\n",
    "print(f\"Eval Loss: {avg_eval_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import frogml\n",
    "from pathlib import Path\n",
    "\n",
    "model = model.module if isinstance(model, torch.nn.DataParallel) else model\n",
    "main_dir = Path.cwd() / \"main\"\n",
    "metrics = {\"eval_loss\": avg_eval_loss}\n",
    "repository = 'nlp-models'\n",
    "model_name = 'sentiment_analysis'\n",
    "\n",
    "frogml.huggingface.log_model(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    repository=repository,\n",
    "    model_name=model_name,\n",
    "    dependencies=[str(main_dir / (\"pyproject.toml\"))],\n",
    "    code_dir=main_dir,\n",
    "    parameters=hyper_parameters,\n",
    "    metrics=metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import frogml.huggingface\n",
    "\n",
    "model, tokenizer = frogml.huggingface.load_model(\n",
    "    repository=repository,\n",
    "    model_name=model_id,\n",
    "    version=\"\" # Model version\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentiment-analysis-P6XkMsq4-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

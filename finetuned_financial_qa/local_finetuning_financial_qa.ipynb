{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70709722",
   "metadata": {},
   "source": [
    "# Fine-Tuning T5 for Financial QA and Logging to JFrog ML Registry\n",
    "\n",
    "This notebook demonstrates fine-tuning a T5 model on financial Q&A data and logging the model version to JFrog ML Registry.\n",
    "\n",
    "**Key Steps:**\n",
    "1. **Setup**: Import libraries and define configuration\n",
    "2. **Data Preparation**: Load and format financial Q&A dataset  \n",
    "3. **Model Fine-Tuning**: Train T5 using transformers directly\n",
    "4. **Evaluation**: Test the fine-tuned model with sample questions\n",
    "5. **Model Logging**: Log the model version to JFrog ML Registry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c714bdc",
   "metadata": {},
   "source": [
    "## 🛠️ Setup and Imports\n",
    "\n",
    "We'll import all the libraries we need for T5 fine-tuning and model logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89c17c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:datasets:TensorFlow version 2.19.0 available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Libraries imported successfully!\n",
      "🎯 Device: CPU\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, TrainingArguments, Trainer\n",
    "from datasets import Dataset\n",
    "import frogml\n",
    "\n",
    "print(\"✅ Libraries imported successfully!\")\n",
    "print(f\"🎯 Device: {'GPU' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787ef91f",
   "metadata": {},
   "source": [
    "## ⚙️ Model Configuration\n",
    "\n",
    "Define all the parameters for our T5 model and training process. You can adjust these values to experiment with different settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a14d8762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Configuration set!\n",
      "📊 model_id: t5-base\n",
      "📊 max_length: 128\n",
      "📊 target_length: 64\n",
      "📊 train_batch_size: 2\n",
      "📊 eval_batch_size: 2\n",
      "📊 learning_rate: 0.0005\n",
      "📊 num_epochs: 1\n",
      "📊 max_samples: 10\n",
      "📊 seed: 42\n",
      "📊 dataset_url: https://qwak-public.s3.amazonaws.com/example_data/financial_qa.csv\n"
     ]
    }
   ],
   "source": [
    "# Model and dataset configuration\n",
    "model_id = \"t5-base\"\n",
    "dataset_name = \"financial_qa_dataset\"\n",
    "\n",
    "# Training parameters\n",
    "config = {\n",
    "    \"model_id\": \"t5-base\",\n",
    "    \"max_length\": 128,  # Shorter sequences = faster training\n",
    "    \"target_length\": 64,  # Shorter outputs = faster training\n",
    "    \"train_batch_size\": 2,  # Smaller batch for CPU\n",
    "    \"eval_batch_size\": 2,\n",
    "    \"learning_rate\": 5e-4,  # Higher LR for faster convergence\n",
    "    \"num_epochs\": 1,  # Just 1 epoch for quick demo\n",
    "    \"max_samples\": 10,  # Only use 100 samples total\n",
    "    \"seed\": 42,\n",
    "    \"dataset_url\": \"https://qwak-public.s3.amazonaws.com/example_data/financial_qa.csv\"\n",
    "}\n",
    "\n",
    "print(\"✅ Configuration set!\")\n",
    "for key, value in config.items():\n",
    "    print(f\"📊 {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d868ec3",
   "metadata": {},
   "source": [
    "## 📊 Data Loading and Preparation\n",
    "\n",
    "Load the financial Q&A dataset and format it for T5 training. We'll add the \"question:\" and \"answer:\" prefixes that T5 expects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f70be78d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Loading financial QA dataset...\n",
      "🚀 Using 10 samples for quick demo\n",
      "✅ Dataset loaded: (10, 7)\n",
      "\n",
      "🔍 Sample:\n",
      "Input: question: For a car, what scams can be plotted with 0% financing vs rebate?\n",
      "Target: answer: The car deal makes money 3 ways. If you pay in one lump payment. If the payment is greater than what they paid for the car, plus their expenses, they make a profit. They loan you the money. You make payments over months or years, if the total amount you pay is greater than what they paid for the car, plus their expenses, plus their finance expenses they make money. Of course the money takes years to come in, or they sell your loan to another business to get the money faster but in a smaller amount. You trade in a car and they sell it at a profit. Of course that new transaction could be a lump sum or a loan on the used car... They or course make money if you bring the car back for maintenance, or you buy lots of expensive dealer options. Some dealers wave two deals in front of you: get a 0% interest loan. These tend to be shorter 12 months vs 36,48,60 or even 72 months. The shorter length makes it harder for many to afford. If you can't swing the 12 large payments they offer you at x% loan for y years that keeps the payments in your budget. pay cash and get a rebate. If you take the rebate you can't get the 0% loan. If you take the 0% loan you can't get the rebate. The price you negotiate minus the rebate is enough to make a profit. The key is not letting them know which offer you are interested in. Don't even mention a trade in until the price of the new car has been finalized. Otherwise they will adjust the price, rebate, interest rate, length of loan,  and trade-in value to maximize their profit. The suggestion of running the numbers through a spreadsheet is a good one. If you get a loan for 2% from your bank/credit union for 3 years and the rebate from the dealer, it will cost less in total than the 0% loan from the dealer. The key is to get the loan approved by the bank/credit union before meeting with the dealer. The money from the bank looks like cash to the dealer.\n"
     ]
    }
   ],
   "source": [
    "# Load and prepare data\n",
    "print(\"📊 Loading financial QA dataset...\")\n",
    "df = pd.read_csv(config[\"dataset_url\"])\n",
    "\n",
    "# Take a very small subset for quick demo (1-2 minutes training)\n",
    "df = df.head(config[\"max_samples\"])\n",
    "print(f\"🚀 Using {len(df)} samples for quick demo\")\n",
    "\n",
    "# Format for T5\n",
    "df['input_text'] = \"question: \" + df['instruction']\n",
    "df['target_text'] = \"answer: \" + df['output']\n",
    "\n",
    "print(f\"✅ Dataset loaded: {df.shape}\")\n",
    "print(\"\\n🔍 Sample:\")\n",
    "print(f\"Input: {df['input_text'].iloc[0]}\")\n",
    "print(f\"Target: {df['target_text'].iloc[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9228e8c",
   "metadata": {},
   "source": [
    "## 🚀 Model Training\n",
    "\n",
    "Load the T5 model, tokenize our data, and start the fine-tuning process. This will take a few minutes depending on your hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41c5ffd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Loading T5 model and tokenizer...\n",
      "🔧 Tokenizing dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 10/10 [00:00<00:00, 319.69 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Training samples: 8\n",
      "📊 Eval samples: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haha/anaconda3/envs/qwak-new-3.11/lib/python3.11/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 00:19, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Training completed!\n"
     ]
    }
   ],
   "source": [
    "# Set random seed\n",
    "torch.manual_seed(config[\"seed\"])\n",
    "np.random.seed(config[\"seed\"])\n",
    "\n",
    "# Load tokenizer and model\n",
    "print(\"🔧 Loading T5 model and tokenizer...\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(config[\"model_id\"])\n",
    "model = T5ForConditionalGeneration.from_pretrained(config[\"model_id\"])\n",
    "\n",
    "# Tokenize the data\n",
    "print(\"🔧 Tokenizing dataset...\")\n",
    "def tokenize_function(examples):\n",
    "    inputs = tokenizer(examples[\"input_text\"], max_length=config[\"max_length\"], truncation=True, padding=\"max_length\")\n",
    "    targets = tokenizer(examples[\"target_text\"], max_length=config[\"target_length\"], truncation=True, padding=\"max_length\")\n",
    "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "# Convert to Hugging Face dataset format\n",
    "from datasets import Dataset as HFDataset\n",
    "dataset = HFDataset.from_pandas(df)\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Split into train/eval\n",
    "train_test_split = tokenized_dataset.train_test_split(test_size=0.2, seed=config[\"seed\"])\n",
    "train_dataset = train_test_split[\"train\"]\n",
    "eval_dataset = train_test_split[\"test\"]\n",
    "\n",
    "print(f\"📊 Training samples: {len(train_dataset)}\")\n",
    "print(f\"📊 Eval samples: {len(eval_dataset)}\")\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=config[\"num_epochs\"],\n",
    "    per_device_train_batch_size=config[\"train_batch_size\"],\n",
    "    per_device_eval_batch_size=config[\"eval_batch_size\"],\n",
    "    learning_rate=config[\"learning_rate\"],\n",
    "    logging_steps=10,  # Log more frequently for small dataset\n",
    "    save_steps=1000,  # Don't save checkpoints for quick demo\n",
    "    eval_steps=50,  # Evaluate more frequently\n",
    "    save_total_limit=1,  # Save space\n",
    "    dataloader_num_workers=0,  # Avoid multiprocessing on CPU\n",
    "    fp16=False,  # Ensure CPU compatibility\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")\n",
    "\n",
    "print(\"🚀 Starting training...\")\n",
    "trainer.train()\n",
    "print(\"✅ Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af658668",
   "metadata": {},
   "source": [
    "## 🧪 Testing the Fine-tuned Model\n",
    "\n",
    "Test our newly trained model with some sample financial questions to see how well it learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "04626ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Testing fine-tuned model...\n",
      "🎯 Sample responses:\n",
      "\n",
      "--- Question 1 ---\n",
      "❓ What is the difference between stocks and bonds?\n",
      "💡 what is the difference between stocks and bonds?\n",
      "\n",
      "--- Question 2 ---\n",
      "❓ How does inflation affect interest rates?\n",
      "💡 inflation affect interest rates.\n",
      "\n",
      "--- Question 3 ---\n",
      "❓ What is compound interest?\n",
      "💡 compound interest?\n",
      "\n",
      "✅ Evaluation completed!\n"
     ]
    }
   ],
   "source": [
    "# Test the fine-tuned model\n",
    "print(\"🧪 Testing fine-tuned model...\")\n",
    "\n",
    "\n",
    "# Force CPU usage to avoid MPS issues\n",
    "model = model.to(\"cpu\")\n",
    "\n",
    "def generate_answer(question, model, tokenizer, max_length=150):\n",
    "    input_text = f\"question: {question}\"\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **input_ids,\n",
    "            max_length=max_length,\n",
    "            num_beams=2,\n",
    "            early_stopping=True,\n",
    "            do_sample=False\n",
    "        )\n",
    "    \n",
    "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return answer\n",
    "\n",
    "# Test questions\n",
    "test_questions = [\n",
    "    \"What is the difference between stocks and bonds?\",\n",
    "    \"How does inflation affect interest rates?\", \n",
    "    \"What is compound interest?\"\n",
    "]\n",
    "\n",
    "print(\"🎯 Sample responses:\")\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    answer = generate_answer(question, model, tokenizer)\n",
    "    print(f\"\\n--- Question {i} ---\")\n",
    "    print(f\"❓ {question}\")\n",
    "    print(f\"💡 {answer}\")\n",
    "\n",
    "print(\"\\n✅ Evaluation completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47b15fc",
   "metadata": {},
   "source": [
    "## 📦 Save to JFrog ML Registry\n",
    "\n",
    "Log our trained model to JFrog ML Registry so we can deploy it later. Make sure to update the file path below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147387e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:frogml.sdk.model_version.utils.model_log_config:No version provided; using current datetime as the version\n",
      "INFO:HuggingfaceModelVersionManager:Logging model finetuned_financial_qa to finance-models\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Logging model to JFrog ML Registry...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:JmlCustomerClient:Customer exists in JML.\n",
      "INFO:JmlCustomerClient:Getting project key for repository finance-models\n",
      "INFO:frogml.sdk.model_version.utils.files_tools:Code directory, predict file and dependencies are not provided. Setup template files for model_name finetuned_financial_qa\n",
      "/private/var/folders/mt/wvz9xr_s7k3cwk3r0b96hyn00000gn/T/tmptpojhukr/finetuned_financial_qa.pretrained_model/special_tokens_map.json: 100%|██████████| 2.54k/2.54k [00:00<00:00, 6.04kB/s]\n",
      "\u001b[A\n",
      "\n",
      "/private/var/folders/mt/wvz9xr_s7k3cwk3r0b96hyn00000gn/T/tmptpojhukr/finetuned_financial_qa.pretrained_model/special_tokens_map.json: 100%|██████████| 2.54k/2.54k [00:00<00:00, 2.98kB/s]\n",
      "\n",
      "\u001b[A\n",
      "\n",
      "/private/var/folders/mt/wvz9xr_s7k3cwk3r0b96hyn00000gn/T/tmptpojhukr/finetuned_financial_qa.pretrained_model/generation_config.json:   0%|          | 0.00/142 [00:00<?, ?B/s]\n",
      "\n",
      "\n",
      "/private/var/folders/mt/wvz9xr_s7k3cwk3r0b96hyn00000gn/T/tmptpojhukr/finetuned_financial_qa.pretrained_model/added_tokens.json: 100%|██████████| 2.59k/2.59k [00:00<00:00, 3.42kB/s]\n",
      "/private/var/folders/mt/wvz9xr_s7k3cwk3r0b96hyn00000gn/T/tmptpojhukr/finetuned_financial_qa.pretrained_model/config.json: 100%|██████████| 1.47k/1.47k [00:00<00:00, 1.82kB/s]\n",
      "/private/var/folders/mt/wvz9xr_s7k3cwk3r0b96hyn00000gn/T/tmptpojhukr/finetuned_financial_qa.pretrained_model/tokenizer_config.json: 100%|██████████| 20.9k/20.9k [00:00<00:00, 21.9kB/s]\n",
      "/private/var/folders/mt/wvz9xr_s7k3cwk3r0b96hyn00000gn/T/tmptpojhukr/finetuned_financial_qa.pretrained_model/generation_config.json: 100%|██████████| 142/142 [00:00<00:00, 327B/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "/private/var/folders/mt/wvz9xr_s7k3cwk3r0b96hyn00000gn/T/tmptpojhukr/finetuned_financial_qa.pretrained_model/generation_config.json: 100%|██████████| 142/142 [00:00<00:00, 188B/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "/private/var/folders/mt/wvz9xr_s7k3cwk3r0b96hyn00000gn/T/tmptpojhukr/finetuned_financial_qa.pretrained_model/spiece.model: 100%|██████████| 792k/792k [00:00<00:00, 849kB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "/private/var/folders/mt/wvz9xr_s7k3cwk3r0b96hyn00000gn/T/tmptpojhukr/finetuned_financial_qa.pretrained_model/model.safetensors: 100%|██████████| 892M/892M [00:52<00:00, 17.0MB/s]\n",
      "/var/folders/mt/wvz9xr_s7k3cwk3r0b96hyn00000gn/T/tmptpojhukr/code.zip: 100%|██████████| 3.26k/3.26k [00:00<00:00, 9.87kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-19 20:43:12,819 - INFO - frogml.storage.logging._log_config.frog_ml.__upload_model:540 - Model: \"finetuned_financial_qa\", version: \"2025-09-19-17-41-57-120\" has been uploaded successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎉 Model logged successfully to JFrog ML Registry!\n"
     ]
    }
   ],
   "source": [
    "# Log model to JFrog ML Registry\n",
    "print(\"📦 Logging model to JFrog ML Registry...\")\n",
    "\n",
    "try:\n",
    "    frogml.huggingface.log_model(   \n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        repository=\"finance-models\",    # The JFrog repository to upload the model to\n",
    "        model_name=\"finetuned_financial_qa\",     # The uploaded model name\n",
    "        version=\"\",     # Optional. The uploaded model version\n",
    "        parameters={\"finetuning-dataset\": dataset_name, \"epochs\": config[\"num_epochs\"]},\n",
    "    )\n",
    "    print(\"🎉 Model logged successfully to JFrog ML Registry!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error logging model: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181257aa",
   "metadata": {},
   "source": [
    "## ✅ Training Workflow Complete!\n",
    "\n",
    "### 🎯 What we accomplished:\n",
    "1. ✅ **Fine-tuned T5** on financial Q&A data locally\n",
    "2. ✅ **Tested model** with sample financial questions  \n",
    "3. ✅ **Logged the tokenizer and model binaries to JFrog ML Registry** with complete metadata\n",
    "4. ✅ **Ready for production deployment** via JFrogML platform\n",
    "\n",
    "### 🚀 Your Model is Now Ready For:\n",
    "- **Container Building**: Automated packaging in JFrogML UI\n",
    "- **Real-time Serving**: Instant financial Q&A API endpoints\n",
    "- **Batch Processing**: Large-scale financial document analysis\n",
    "- **A/B Testing**: Compare with other model versions\n",
    "- **Team Collaboration**: Share with colleagues via registry\n",
    "\n",
    "**🎉 Congratulations!** Your Financial QA model is ready for the next stage of the ML lifecycle! 🚀"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qwak-new-3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
